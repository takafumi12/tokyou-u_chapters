{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def main():\n",
    "  # data load\n",
    "    df = pd.read_csv(\"/root/userspace/Workspace/competition2/input/train.csv\", header=0)\n",
    "\n",
    "    \n",
    "    \n",
    "  # load score data\n",
    "    dfs = pd.read_csv(\"/root/userspace/Workspace/competition2/input/test.csv\", header=0)\n",
    "    \n",
    "    y_train = df.iloc[:,-1]\n",
    "    X_train = df.iloc[:,:-1]\n",
    "    \n",
    "    Xs = dfs.iloc[:,:]  \n",
    "    \n",
    "     # preprocessing-1: null imputation\n",
    "    imp = Imputer(strategy = 'mean', axis = 0)\n",
    "    imp.fit(X_train)\n",
    "    X_train = pd.DataFrame(imp.transform(X_train))\n",
    "    Xs = pd.DataFrame(imp.transform(Xs))\n",
    "    \n",
    "     # RFECV\n",
    "    #X_train_columns = X_train.columns.values\n",
    "    #print(X_train.columns.values)\n",
    "    #selector = RFECV(estimator=GradientBoostingRegressor(n_estimators=100,random_state=0), step=0.05)\n",
    "    #selector.fit(X_train, y_train)\n",
    "    #X_train = selector.transform(X_train)\n",
    "    #X_train = pd.DataFrame(X_train, columns=X_train_columns[selector.support_])\n",
    "    #print(X_train.columns.values)\n",
    "      \n",
    "    #Xs.loc[:,list(set(X_train_columns)-set(Xs.columns.values))] = \\\n",
    "    #Xs.loc[:,list(set(X_train_columns)-set(Xs.columns.values))].fillna(0, axis=1)\n",
    "    #Xs = Xs.drop(list(set(Xs.columns.values)-set(X_train_columns)), axis=1)    \n",
    "    \n",
    "  # modeling\n",
    "    best_model_score = -1000\n",
    "    best_model = {}\n",
    "    scores = {}\n",
    "    for pipe_name, pipeline in pipelines.items():\n",
    "        if pipe_name == 'cat':\n",
    "            gs = GridSearchCV(pipeline, params, cv = 10,scoring = 'neg_mean_squared_error',return_train_score=False)\n",
    "            gs.fit(X_train, y_train)\n",
    "            result = np.average(cross_val_score(gs, X_train, y_train, scoring='neg_mean_squared_error', cv=5))\n",
    "            scores[(pipe_name,'train')] = result\n",
    "        \n",
    "        #if pipe_name == 'xgb' or pipe_name == 'lgb':\n",
    "            #gs = GridSearchCV(pipeline, params, cv = 10,scoring = 'neg_mean_squared_error',return_train_score=False)\n",
    "            #gs.fit(X_train, y_train)\n",
    "            #result = np.average(cross_val_score(gs, X_train, y_train, scoring='neg_mean_squared_error', cv=5))\n",
    "            #scores[(pipe_name,'train')] = result\n",
    "        \n",
    "        #if pipe_name == 'rf' or pipe_name == 'gb':\n",
    "            #gs = GridSearchCV(estimator=pipeline,\n",
    "                  #param_grid=param_grid_class,\n",
    "                  #scoring='neg_mean_squared_error',\n",
    "                  #cv=10,\n",
    "                  #return_train_score=False)\n",
    "            #gs.fit(X_train, y_train)\n",
    "            #result = np.average(cross_val_score(gs, X_train, y_train, scoring='neg_mean_squared_error', cv=5))\n",
    "            #scores[(pipe_name,'train')] = result\n",
    "        else:\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            result = np.average(cross_val_score(pipeline, X_train, y_train, scoring='neg_mean_squared_error', cv=5))\n",
    "            scores[(pipe_name,'train')] = result\n",
    "        if result > best_model_score:\n",
    "            best_model_score = result\n",
    "            if pipe_name == 'cat' or pipe_name == 'xgb' or pipe_name == 'lgb' or pipe_name == 'rf' or pipe_name == 'gb':\n",
    "                best_model = gs\n",
    "            else:\n",
    "                best_model = pipeline   \n",
    "    print(best_model)\n",
    "    print(sorted(scores.items(), key=lambda x:x[1], reverse=True))\n",
    "        \n",
    "        # scoring（後で変更）\n",
    "    #predict = best_model.predict(Xs)\n",
    "    #print(predict)\n",
    "    score = pd.DataFrame(best_model.predict(Xs), columns=['quality'])\n",
    "    score.to_csv(\"/root/userspace/Workspace/competition2/pred.csv\", index=False)\n",
    "    print(score)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # CLASSIFIER\n",
    "    pipelines = {\n",
    "        #'knn':Pipeline([('scl',StandardScaler()),\n",
    "                  #('est',KNeighborsRegressor())]),\n",
    "        #'linear':Pipeline([('scl',StandardScaler()),\n",
    "                  #('est',LinearRegression())]),\n",
    "        #'tree':Pipeline([('scl',StandardScaler()),\n",
    "                  #('est',DecisionTreeRegressor(random_state=1))]),\n",
    "        #'rf':Pipeline([('scl',StandardScaler()),\n",
    "                  #('est',RandomForestRegressor(random_state=1))]),\n",
    "        #'gb':Pipeline([('scl',StandardScaler()),\n",
    "                  #('est',GradientBoostingRegressor(random_state=1))]),\n",
    "        #'mlp':Pipeline([('scl',StandardScaler()),\n",
    "                  #('est',MLPRegressor(hidden_layer_sizes=(3,3),max_iter=1000,random_state=1))]),\n",
    "        #'xgb':Pipeline([('scl',StandardScaler()),\n",
    "                  #('est',xgb.XGBRegressor(random_state=1))]),\n",
    "        #'lgb':Pipeline([('scl',StandardScaler()),\n",
    "                  #('est',lgb.LGBMRegressor(random_state=1))]),\n",
    "        'cat':Pipeline([('scl',StandardScaler()),\n",
    "                  ('est',CatBoostRegressor(random_state=1))])\n",
    "    }\n",
    "    \n",
    "    param_grid_class = {'est__n_estimators':[1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]}\n",
    "    params = {'est__depth': [2, 4, 6, 8],'est__learning_rate' : [0.03, 0.1, 0.15],'est__l2_leaf_reg': [1,3,7],'est__iterations': [300]}\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
